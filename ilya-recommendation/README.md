**NOTE**: this is *told* to be the list Ilya Sutskever suggested to John Carmack but I couldn't find any evidence of it as of my research. 

Despite the lack of evidence, still looks like a good fundamental list for people interested in deep learning.

<br>

# Ilya Recommendations

This table orders the resources from the [Tensor Labbet AI Reading List](https://tensorlabbet.com/2024/09/24/ai-reading-list/), reportedly suggested by Ilya Sutskever to John Carmack, for beginners in deep learning. 

It progresses from foundational to advanced topics across CNNs, RNNs, Transformers, Information Theory, and Miscellaneous categories.

<br>

| #  | Resource | Description |
|----|----------|-------------|
| 1  | [CS231n Course](https://cs231n.github.io/) | Stanford course on CNNs, covering basics of neural networks and image recognition. |
| 2  | [AlexNet Paper](https://dl.acm.org/doi/abs/10.1145/3065386) | Introduces deep CNNs for image classification, a foundational work in the field. |
| 3  | [ResNet Paper (2015)](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) | Presents residual connections for training deeper networks. |
| 4  | [Dilated Convolutions Paper (2015)](https://arxiv.org/abs/1511.07122) | Enhances CNN receptive fields with dilated convolutions. |
| 5  | [ResNet Identity Mappings Paper (2016)](https://arxiv.org/abs/1603.05027) | Refines ResNet with identity mappings for improved performance. |
| 6  | [Understanding LSTM Networks Blog Post](https://colah.github.io/post/2015-08-Understanding-LSTMs/) | Accessible intro to LSTMs, a key RNN type for sequential data. |
| 7  | [The Unreasonable Effectiveness of RNNs Blog Post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) | Explores RNN applications with practical examples. |
| 8  | [RNN Regularization Paper](https://arxiv.org/abs/1409.2329) | Discusses techniques for training stable RNNs. |
| 9  | [Deep Speech 2 Paper](http://proceedings.mlr.press/v48/amodei16.html) | Applies RNNs to end-to-end speech recognition. |
| 10 | [RNNsearch Paper](https://arxiv.org/abs/1409.0473) | Focuses on RNNs for machine translation with attention mechanisms. |
| 11 | [Pointer Networks Paper](https://papers.nips.cc/paper_files/paper/2015/file/29921001f2f04bd3baee84a12e98098f-Paper.pdf) | Introduces an RNN architecture for combinatorial optimization. |
| 12 | [Set2Set Paper](https://arxiv.org/abs/1511.06391) | Presents an RNN architecture for set-to-set mapping. |
| 13 | [Relation Networks Paper](https://papers.nips.cc/paper_files/paper/2017/hash/e6acf4b0f69f6f6e60e9a815938aa1ff-Abstract.html) | Explores relational reasoning with RNNs. |
| 14 | [Relational Recurrent Neural Networks Paper](https://papers.nips.cc/paper/2018/hash/e2eabaf96372e20a9e3d4b5f83723a61-Abstract.html) | Advances RNNs for relational tasks. |
| 15 | [Neural Turing Machines Paper](https://arxiv.org/abs/1410.5401) | Extends RNNs with external memory capabilities. |
| 16 | [Attention Is All You Need Paper](https://user.phil.hhu.de/~cwurm/wp-content/uploads/2020/01/7181-attention-is-all-you-need.pdf) | Introduces the Transformer architecture, revolutionizing sequence modeling. |
| 17 | [The Annotated Transformer Blog Post](https://nlp.seas.harvard.edu/annotated-transformer/) | Detailed walkthrough of the Transformer paper. |
| 18 | [Scaling Laws for Neural Language Models Paper](https://arxiv.org/abs/2001.08361) | Analyzes how Transformers scale with data and compute. |
| 19 | [The First Law of Complexodynamics Blog Post](https://scottaaronson.blog/?p=762) | Introduces complexity concepts in an accessible way. |
| 20 | [A Tutorial Introduction to the Minimum Description Length Principle](https://arxiv.org/abs/math/0406077) | Explains MDL, a key concept in information theory for model selection. |
| 21 | [Kolmogorov Complexity and Algorithmic Randomness Book Chapter](https://www.lirmm.fr/~ashen/kolmbook-eng-scan.pdf) | Deep dive into Kolmogorov complexity and randomness. |
| 22 | [Quantifying the Rise and Fall of Complexity in Closed Systems Paper](https://arxiv.org/abs/1405.6903) | Examines complexity dynamics in closed systems. |
| 23 | [Machine Super Intelligence Dissertation](https://sonar.ch/usi/documents/317954) | Theoretical exploration of general AI and superintelligence. |
| 24 | [Keeping Neural Networks Simple by Minimizing the Description Length](https://www.cs.toronto.edu/~hinton/absps/colt93.pdf) | Links neural network regularization to information theory. |
| 25 | [Variational Lossy Autoencoder Paper](https://arxiv.org/abs/1611.02731) | Covers autoencoders for representation learning and compression. |
| 26 | [GPipe Paper](https://arxiv.org/abs/1811.06965) | Discusses efficient distributed training for large neural networks. |
| 27 | [Neural Message Passing for Quantum Chemistry Paper](https://arxiv.org/abs/1704.01212) | Applies neural networks to quantum chemistry, an advanced application. |