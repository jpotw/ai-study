# 선형 회귀(Linear Regression)

<br>

레이어 함수에는 `named_parameters()`라는 공통 함수가 있음.

`named_parameters()`는 (name, tensor) 형태의 튜플들을 반환함. 이때 name은 텐서의 이름(Ex. weight, bias)이고, tensor는 텐서 자체를 말함.

레이어 함수의 경우 기본으로 requires_grad=True 옵션이 설정되어 있음

`nn.init.constant_()`을 통해 레이어 함수 내의 파라미터의 초기값을 수동으로 설정할 수도 있음.

다중회귀에서 Weight은 입력값x출력값 만큼 존재(이때 행렬상 대칭)하고 Bias는 출력값 만큼 존재함.

  
PyTorch의 nn.Module을 상속받은 클래스들은 __call__ 메서드 내에서 forward 메서드를 호출하도록 구현되어 있음


<br>

사실 개념 자체는 매우 상식적이고 간단한데 코드로 구현하다 보니 조금 복잡해보임 -> 이름(변수, 메서드명)들을 기준으로 flow를 이해하는 것이 중요해보임.
지금까지 배운 것을 자주 사용하는 변수 이름과 함께 정리해보면 대략 다음과 같은 흐름으로 진행됨:

*(지도 학습, 선형회귀 기준)*

  
1. 입력 변수와 출력변수 - **inputs, labels**
2. 입력과 출력의 차원 수 정의 - **n_input, n_output**
3. 예측 함수 정의 - **Net**
  - Net은 커스텀 클래스다.
  - Net은 input과 output을 인수로 가진 함수이다.
  - Net의 forward() 메서드를 통해 한 개 이상의 레이어를 결합한다.
  - 이후 net (Net의 인스턴스)을 호출할 때 __call__ 메서드가 자동으로 forward() 메서드를 호출하게 되면서 계산과정을 기록하는 동시에(backpropagation에서 쓰일 예정) 예측값을 계산한다.
4. 예측함수를 통해 예측값 계산 - **outputs** =net(inputs)
  - *주의: outputs는 실제 결과(labels)가 아닌 예측함수의 결과값임*
  - 이때 inputs와 outputs는 numpy 배열로 정의돼 있기 때문에 모델 학습을 위해서는 텐서로 변환해야 함.
  - outputs의 경우 추가적으로 view() 메서드를 통해 텐서의 형태를 (-1,1)로 변환해야 함 (짝 맞추기)
      - output은 하나의 값(scalar)이므로 하나의 차원을 추가해야 함 (Ex. `[정답1, 정답2, 정답3]` -> `[[정답1], [정답2], [정답3]]`)
5. 손실 함수 정의 - **criterion** = nn.MSELoss()
6. 손실 함수를 통해 손실값 계산 - **loss** = criterion(outputs, labels)
7. 손실의 경사 계산 - **loss.backward()**
8. 최적화 함수 설정 - **optimizer** = optim.SGD(net.parameters(), lr=0.01)
    - 학습률(learning rate)은 최적화 함수에서 가장 중요한 부분이다.
    - 학습률이 너무 크면 최적화가 잘 안되고 너무 작으면 최적화가 너무 오래 걸린다.
9. 손실값을 최소화하는 방향으로 파라미터 업데이트, 초기화 - **optimizer.step(), optimizer.zero_grad()**
10. 1~9의 반복 - **epochs** = n, for epoch in range(epochs):


cf) 입력 차원이 2개 이상인 다중 함수인 경우에도 하나의 nn.Linear()에서 처리 가능하다