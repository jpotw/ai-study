# 이진 분류(Binary Classification)


로지스틱 회귀 != 회귀
로지스틱 회귀는 분류와 같다고 생각하면 됨.

<br>

## 분류와 회귀의 차이점:

(*주요 keyword: 시그모이드 함수, 교차 엔트로피 함수*)

<br>

### 1. 판단에 대한 기준 관련

**정확도**
지도 학습에는 크게 분류(classification)와 회귀(regression)가 있다.
회귀는 실제 정답의 근사치를 구하는 목적으로 사용하고 분류는 어느 집단에 속하는지 판단하는 목적으로 사용한다.

분류(classification)는 회귀(regression)와 다르게 **정답과 오답에 대한 분명한 기준**이 있다.

때문에 '**정확도**'라는 하나의 개념이 추가된다.

정확도 = 예측해서 맞힌 데이터 수 / 전체 데이터 수

<br>

### 2. 예측함수(**Net**) 관련
  
**시그모이드 함수 (`nn.Sigmoid()`)**

Binary Classification 의 경우, 기존의 회귀 모델에서 보았던 선형 함수(nn.Linear())에 시그모이드 함수(sigmoid)를 추가한 것이다.
이때 sigmoid 함수는 함수의 값을 확률로 해석하는 데 도움을 준다.

<br>

### 3. 손실함수(**criterion**) 관련

**교차 엔트로피 함수 (`nn.BCELoss()`)**

평균제곱오차(MSE)를 사용하는 회귀 모델과 달리 이진 분류 모델에서는 교차 엔트로피 함수(Cross Entropy)를 사용한다.

교차 엔트로피 함수는 모든 데이터에 대한 확신도를 곱하여 가장 확신도가 높은 데이터를 찾는다.



## **이진 분류의 학습 과정**


### 1. **반복 계산을 위한 모델 정의 및 초기화 처리**

1. 예측함수(Net)
2. 손실함수(criterion)
3. 최적화 함수(optimizer)
4. 학습률(learning rate)
5. 반복 횟수(num_epochs)
6. (기록용 리스트(history))

Example:

```python
lr = 0.01
num_epochs = 100
net = Net(n_input, n_output)
criterion = nn.BCELoss()
optimizer = optim.SGD(net.parameters(), lr=lr)
history = np.zeros((num_epochs, 5))
```

<br>

### 2. **훈련 과정**

1. 경사값 초기화 (zero_grad())
2. 예측값 계산 (outputs)
3. 손실값 계산 (loss)
4. 경사값 계산 (backward propagation) (loss.backward())
5. 파라미터 갱신 (optimizer.step())

```python
for epoch in range(num_epochs):
  optimizer.zero_grad()
  outputs = net(inputs)
  loss = criterion(outputs, labels)
  loss.backward()
  optimizer.step()

# history 기록용
  train_loss = loss.item()
  predicted = torch.where(outputs < 0.5, 0, 1)
  train_acc = (predicted == labels).sum() / len(y_train)
```

<br>

### 3. **평가(검증) 과정**

```python
outputs_test = net(inputs_test)
test_loss = criterion(outputs_test, labels_test)
val_loss = loss_test.item()
predicted_test = torch.where(outputs_test < 0.5, 0, 1)
val_acc = (predicted_test == labels_test).sum() / len(labels_test)

if epoch % 10 == 0:
  print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")
  item = np.array([epoch, train_loss, train_acc, val_loss, val_acc])
  history = np.vstack((history, item))
```

<br>


## 주의할 점

그러나 `BCELoss()` 함수보다 `BCEWithLogitsLoss()` 함수를 사용하는 것이 더 안정적이다.

`BCELosss()`는 예측 함수에서 시그모이드 함수를 적용하고 이후 손실 함수에서 교차 엔트로피 함수를 적용하는 방식이다 -> 시그모이드 함수와 교차 엔트로피 함수를 독립적으로 계산하는 것은 불안정하다. 

그래서 PyTorch에서는 예측 함수에서는 시그모이드 함수를 적용하지 않고 손실 함수에서 시그모이드 함수를 적용하는 방식이 권장된다.

단 여기서 주의할 점은 0과 1을 분류할 때의 기준이** 0.5가 아니라 0.0**이라는 것이다. 

`BCELoss()`를 쓰든 `BCEWithLogitsLoss()`를 쓰든 **sigmoid 적용 후 0.5를 기준으로 이진분류**를 하는 것인데 sigmoid(0) = 0.5이므로 sigmoid를 손실 함수에 사용할 때는 예측함수의 출력값을 0을 기준으로 판단해야 하는 것이다.



<br>

### cf) 로그함수의 불안정성:  

1. 정의역 문제: 로그 함수(log)는 0 또는 음수 값을 입력으로 받을 수 없음. (자연로그 함수의 경우) 만약 0에 가까운 작은 값이나 음수가 입력으로 들어오면, 계산 결과가 무한대나 정의되지 않은 값이 될 수 있음. 이는 컴퓨터에서 수를 표현하는 방식의 한계 때문에 문제가 될 수 있음.

  

2. 작은 값에 민감: 로그 함수는 입력 값이 작아질수록 출력 값의 변화가 급격하게 커짐. 특히, 0에 가까운 작은 값의 로그 값은 매우 큰 음수가 됨

  

3. 곱셈을 덧셈으로 변환: 로그 함수의 중요한 특징 중 하나는 곱셈을 덧셈으로 변환한다는 것임. 작은 확률 값들을 계속 곱해나가면 컴퓨터에서 표현 가능한 범위 아래로 값이 매우 작아져 언더플로우 문제가 발생할 수 있음.


  

cf) numpy에서는 직접적인 관련성이 없더라도 array의 길이(shape)만 같다면 이런 식으로 사용할 수 있다고 함.

<br>

```python

x_t0 = x_train[y_train == 0]

```



#### cf) train_test_split 함수

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    x_data, y_data, train_size=70)
```

사용 목적: 데이터를 훈련 데이터와 검증 데이터로 나누는 동시에 섞음.
