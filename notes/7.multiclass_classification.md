# 다중 분류(Multiclass Classification)

이진 분류 복습: **Sigmoid 함수를 통해 0과 1 사이의 확률값을 출력하고 이를 통해 0 또는 1로 분류하는 방법** 이었음

다중 분류는 3개 이상의 클래스로 나누는 것을 말함. -> 각 클래스에 대한 분류기를 만들고 가장 높은 확률값을 갖는 클래스 분류기를 선택

각 출력마다 분류기가 있기 때문에 **행렬**로 표현됨


sigmoid 함수 대신 softmax 함수 사용
softmax 함수는 입력 중 가장 큰 값의 확률을 더 높게, 나머지 값의 확률을 더 낮게 만들어 줌 (완만하게 최댓값을 냄)



cf) fill 대신 fill_() 사용하는 이유

Net 안의 nn.Linear()의 weight와 bias 를 초기화할 때 `fill_()`을 사용했음.

PyTorch 내에서는 `fill()`함수 또한 존재함. 그러나 `fill_()`은 새로운 텐서를 생성하지 않고 기존 텐서를 수정하는 인플레이스 연산용 함수임. 따라서 메모리 사용이 효율적임.

Python의 int와 같은 경우는 불변 객체이기 때문에 인플레이스 연산이 불가함.


## 다중 분류의 학습 과정

### 1. 모델 정의 및 초기화 처리

```python
# n_input, n_output 정의
# Net 정의
net = Net(n_input, n_output)
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=lr)
history = np.zeros((num_epochs, 5))
```

<br>

### 2. 훈련 과정

cf) 분류에서 결과는 이산적인 정수값으로 표현되는 경우가 대부분 -> label의 데이터는 long() 사용

