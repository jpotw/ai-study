# 다중 분류(Multiclass Classification)

이진 분류 복습: **Sigmoid 함수를 통해 0과 1 사이의 확률값을 출력하고 이를 통해 0 또는 1로 분류하는 방법**이었음

<br>

다중 분류는 3개 이상의 클래스로 나누는 것을 말함
- 각 클래스에 대한 분류기를 만들고 가장 높은 확률값을 갖는 클래스 분류기를 선택
- 각 출력마다 분류기가 있기 때문에 **행렬**로 표현됨

<br>

sigmoid 함수 대신 softmax 함수 사용:
- softmax 함수는 입력 중 가장 큰 값의 확률을 더 높게, 나머지 값의 확률을 더 낮게 만들어 줌 (완만하게 최댓값을 냄)

<br>

### 교차 엔트로피 함수

Binary Classification과 같이 손실 함수로 사용되나 수식이 조금 달라진다.

<br>

Multiclass Classification에서는 소프트맥스 함수의 출력을 모두 구한 후 로그를 적용한 다음, 그 중 가장 큰 값을 선택하는 방식으로 사용됨.

과정을 정리해보면:
1. 예측함수: nn.Linear + softmax
2. 손실함수: 예측함수 output에 log 적용 + 정답요소 추출

<br>

### PyTorch에서의 교차 엔트로피 함수

그러나 PyTorch의 내장 함수 중 교차 엔트로피 함수의 수식적 의미와 동등한 함수는 존재하지 않음
- 이유: Chapter 6의 로그함수 불안정성 문제 참고

<br>

따라서 PyTorch에서는:
1. **예측함수: nn.Linear**
2. **손실함수: nn.CrossEntropyLoss (softmax 함수 포함)**
로 구현한다.

<br>

### fill 대신 fill_() 사용하는 이유

Net 안의 nn.Linear()의 weight와 bias를 초기화할 때 `fill_()`을 사용했음.

- PyTorch 내에서는 `fill()`함수 또한 존재함
- 그러나 `fill_()`은 새로운 텐서를 생성하지 않고 기존 텐서를 수정하는 인플레이스 연산용 함수임
- 따라서 메모리 사용이 효율적임
- Python의 int와 같은 경우는 불변 객체이기 때문에 인플레이스 연산이 불가함

<br>

## 다중 분류의 학습 과정

### 1. 모델 정의 및 초기화 처리

<br>

#### 1.1 데이터 불러오기
```python
# 학습용 데이터 준비

# 라이브러리 임포트
from sklearn.datasets import load_iris

# 데이터 불러오기
iris = load_iris()

# 입력 데이터와 정답 데이터
x_org, y_org = iris.data, iris.target

# 입력 데이터 중 0번째와 2번째 열만 선택
x_select = x_org[:,[0,2]]

# 학습용 데이터와 테스트용 데이터 분리
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(
    x_select, y_org, train_size=75, test_size=75,
    random_state=123
)
print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)

# 각 클래스별 데이터 추출
x_t0 = x_train[y_train==0]
x_t1 = x_train[y_train==1]
x_t2 = x_train[y_train==2]

# 데이터 확인 (시각화)
plt.scatter(x_t0[:,0], x_t0[:,1], marker='x', c='k', s=50, label='0 (setosa)')
plt.scatter(x_t1[:,0], x_t1[:,1], marker='o', c='b', s=50, label='1 (versicolor)')
plt.scatter(x_t2[:,0], x_t2[:,1], marker='+', c='k', s=50, label='2 (virginica)')

plt.xlabel('sepal length')
plt.ylabel('petal_length')
plt.legend()
plt.show()
```

<br>

#### 1.2 input, output 크기 정의

```python
n_input = x_train.shape[1]
n_output = len(list(set(y_train)))
```

<br>

#### 1.3 모델 정의

```python
# Net 정의
class Net(nn.Module):
    def __init__(self, n_input, n_output):
        super(Net, self).__init__()
        self.l1 = nn.Linear(n_input, n_output)

        self.l1.weight.data.fill_(0.0)
        self.l1.bias.data.fill_(0.0)

    def forward(self, x):
        x = self.l1(x)
        return x

# 인스턴스 생성
net = Net(n_input, n_output)

# 모델 파라미터 확인
for parameter in net.named_parameters():
    print(parameter)
```

> recap) Net 클래스는 PyTorch에서 예측함수로써 사용되며 `__init__()`과 forward() 메서드로 구성된 custom class이다.

<br>

#### 1.4. 최적화 알고리즘(Optimizer) 및 손실함수(Loss Function) 정의

```python
criterion = nn.CrossEntropyLoss()
lr = 0.01
optimizer = torch.optim.SGD(net.parameters(), lr=lr)
```


<br>

### 2. 경사도 하강법 적용

> cf) 분류에서 결과는 이산적인 정수값으로 표현되는 경우가 대부분 -> label의 데이터는 long() 사용

<br>

#### 2.1 데이터 텐서화
```python
x_train = torch.tensor(x_train).float()
y_train = torch.tensor(y_train).long()

x_test = torch.tensor(x_test).float()
y_test = torch.tensor(y_test).long()
```

#### 2.2. 손실 계산

```python
outputs = net(inputs)
loss = criterion(outputs, labels)

# 손실 계산 그래프 시각화
g = make_dot(loss, params=dict(net.named_parameters()))
display(g)
```

#### 2.3. 반복 계산

```python
num_epochs = 1000
history = np.zeros((0,5))

for epoch in range(num_epochs):
    optimizer.zero_grad()
    outputs = net(x_train)
    loss = criterion(outputs, y_train)
    loss.backward()
    optimizer.step()
    predicted = torch.max(outputs, 1)[1]
```

#### cf) predicted에 관하여

torch.max(outputs, 1)[1]은 [0]에서 예측값을 추출하고 [1]에서 예측값의 인덱스를 추출한다.

해당 케이스에서 예측값의 인덱스가 곧 y_train과 같으므로 [1]만 추출하면 되는 것이다.

#### cf) 모델(예측함수)에 입력을 통과시켜 얻은 출력으로부터 확률 값을 얻고 싶은 경우

net() 에 torch.nn.Softmax(dim=1) 적용하면 됨.



<br>

### softmax 함수를 거치는 이유

softmax 함수를 거치기 전에도 최대인 값이 예측값이 되는데 왜 softmax 함수를 거치는가?

1. 확률로 표현할 수 있음
2. 상대적 차이 표현 가능
3. 학습 안정성
4. **미분 가능성**

정리하면 확률 기반의 안정적인 학습을 위해 softmax 함수를 거치는 것임.

<br>

#### NLLLoss() 함수에 관하여

`NLLLoss()` 함수는 Negative Log Likelihood Loss라는 뜻으로 모델의 예측값에 SoftMax를 적용한 값에 대하여 두 가지 일을 수행한다:

1. 음의 로그를 씌운다
   - `-log0`에 가까울수록 값이 커지고(무한에 가까워지고) `-log1`에 가까울수록 0이 된다

2. 정답 레이블과 예측값을 비교하여 손실값을 계산한다
   - 모델이 높은 확신(Ex. 0.7)로 예측한 경우 -log(0.7)이 되어 값이 상대적으로 작아짐
   - 모델이 낮은 확신(Ex. 0.1)로 예측한 경우 -log(0.1)이 되어 값이 커짐
   - 값의 크기만큼 손실이 크다 == 경사하강의 폭이 넓어진다

