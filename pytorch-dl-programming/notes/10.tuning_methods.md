# 딥러닝 분류 모델의 튜닝 기법

> 이미지를 대상으로 한 딥러닝 분류 모델의 튜닝 기법은 크게 다음 세 가지로 나눌 수 있다.
> 
> 1. 신경망의 다층화
> 2. 최적화 함수 개선
> 3. 과학습에 관한 대책

<br>

## 1. 신경망의 다층화

신경망의 계층을 깊게 만들수록 일반적으로 모델의 범용성이 높아지고 정확도가 높아진다. (Ex. ResNet)

물론 계층이 깊어짐에 따라 경사 소실이나 local minimum에 빠지는 문제가 발생할 수 있지만 이를 해결해 나가면서 층을 깊게 쌓으려는 노력은 계속돼 왔다.

이번 장에서는 계층을 깊게 쌓으면서 생기는 '문제'와 이를 어떻게 '해결'하는지에 대해 알아보자.
<br>

## 2. 대표적인 최적화 함수(optimizer)들

### 2-1. SGD(Stochastic Gradient Descent)

```python
lr = 0.01
W -= lr * W.grad
B -= lr * B.grad
```

### 2-2. Momentum

SGD의 parameter로 추가할 수 있는 것으로 과거에 계산한 경사값을 기억했다가 그만큼 파라미터를 일정 비율 감소시키는 역할을 한다.

**추가 설명:**  
Momentum은 SGD를 개선한 최적화 방법임. 기존 SGD의 문제점을 해결하기 위해 **관성**이라는 개념을 도입했음.

**작동 방식:**  
현재 계산된 경사(gradient)뿐만 아니라 이전 경사들의 **운동량(momentum)**을 유지하면서 파라미터를 업데이트한다.

momentum parameter(보통 0.9로 설정)는 이전 경사들의 반영 비율을 결정한다.

**Q. '반영'한다는 게 정확히 뭔데?**

말 그대로 일정 비율만큼 이전의 경사의 평균을 반영한다는 뜻이다.

Ex. momentum = 0.9이면 다음 경사를 계산할 때 이전의 경사의 90%만큼 반영한다는 뜻이다. (10%의 경사만 현재 단계의 경사를 반영)

<br>

```python
# Momentum SGD
learning_rate = 0.1
momentum = 0.9
velocity = 0  # 이전 속도를 저장하는 변수

def momentum_update(parameter, gradient):
    global velocity
    # 새로운 속도 계산: (이전 속도 * momentum) - (학습률 * 현재 경사)
    velocity = momentum * velocity - learning_rate * gradient
    # 파라미터 업데이트
    parameter = parameter + velocity
    return parameter
```

이렇게 하면 두 가지 장점이 있다:

1. 학습이 한 방향으로 일관되게 진행될 때 **더 빠르게 수렴**한다
2. local minimum에 빠지는 것을 방지하는 데 도움을 준다.

```python
optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)
```

### 2-3. Adam

Adam은 SGD와 Momentum보다 더 복잡한 최적화 알고리즘으로, 학습률을 자동으로 조정하는 기능이 있다. 그래서 대부분의 경우 기본값으로도 잘 작동하며, SGD나 Momentum에 비해 learning rate 설정에 덜 민감하다.

```python
optimizer = optim.Adam(net.parameters())
```

> **cf) Adam에서는 왜 lr을 명시하지 않는가?**  
> Adam의 기본 lr = 0.001 이어서 따로 저장하지 않음.

<br>

## 3. 과학습에 대한 대표적인 대응 방법

### 3-1. Dropout

**훈련 단계:**

| 단계 | 설명 |
|------|------|
| **Forward** | 일부 뉴런을 0으로 만들고 남은 뉴런은 `1/(1 - p)`로 스케일링. |
| **Backward** | 비활성화된 뉴런의 그래디언트는 0 → 가중치 업데이트 X |
| **평균 유지** | 스케일링을 통해 드롭아웃 적용 전후의 출력 평균을 일치시킴. |

**예측 단계:**

- **드롭아웃 비활성화**: 테스트 시에는 모든 뉴런을 사용한다.
- **가중치 보정**: 훈련 시 스케일링(1/(1 - p))을 적용했으므로, 테스트 시에는 가중치에 (1 - p)를 곱해 보정한다. -> 이를 통해 총 출력값이 변하지 않도록 함.

  (예: dropout_ratio = 0.2 → 가중치에 0.8을 곱함)

> **과학습에 대해 분명 강한 효과가 있지만 학습에 소요되는 시간은 전보다 길어진다.**

<br>

### 3-2. Batch Normalization

이전 layer 함수의 출력에 batch normalization layer 함수를 추가해주면 학습 효율 향상 및 과학습 예방이 된다.

conv 함수 뒤에는 `nn.BatchNorm2d` (parameter: 입력 데이터의 채널 수), 선형 함수 뒤에는 `nn.BatchNrom1d` (parameter: 데이터의 차원 수)를 사용한다.

**Batch Normalization이 주로 Convolution layer 뒤에 추가되는 이유와 Dropout이 주로 Pooling layer 뒤에 추가되는 이유**

- **Convolution 연산**은 입력 데이터의 공간적 특성을 추출하는 과정에서 출력값의 분포가 크게 변할 수 있다. Batch Normalization은 이러한 출력값의 분포를 정규화하여, 이후의 레이어가 더 안정적으로 학습할 수 있도록 돕는다.

- Batch Normalization은 **활성화 함수(ReLU 등)** 이전에 적용되기도 한다. 이는 정규화된 데이터가 활성화 함수를 통과할 때 더 효과적으로 작동하도록 하기 위한 것이다.

- **Pooling Layer**는 입력 데이터의 차원을 줄이는 역할을 한다. Pooling 이후에는 데이터의 차원이 줄어들고 중요한 특징들이 추출된 상태이기 때문에 이 시점에서 Dropout을 적용하면 모델이 중요한 특징들에 과도하게 의존하는 것을 방지할 수 있다.

- **Fully Connected Layer (전결합형 레이어)** 앞에 Dropout을 적용하는 경우도 많다. FC 레이어는 파라미터가 많아 과적합이 발생하기 쉬우므로, Dropout을 통해 이를 방지한다.

**BN은 parameter를 가진 layer function 이다.**

BN은 이전 layer의 output channel 수를 input channel 수로 받는다. 

이때 **같은 input channel 수를 갖는다고 하더라도 각각을 독립적인 인스턴스로 정의**해야 한다.

<br>

### 3-3. Data Augmentation

학습을 하기 전 입력 데이터를 인위적으로 가공 -> 학습 데이터의 다양성을 증가시키는 법.

Transforms에서 제공하는 다양한 기능들을 통해 쉽게 적용 가능하다.

정확한 training data 가 아닌 약간씩 가공된 data를 사용하기 때문에 과학습이 되기 어렵다.

<br>

**`nn.Conv2d` padding parameter에 관하여**

nn.Conv2d 설정할 때 padding option을 줄 수 있다.

kernel size가 3x3 인 `Conv2d`에 `padding(1,1)`을 주면 **입력텐서의 경계면(상하좌우)에 dummy data를 하나씩 추가하여 convolution을 진행**한다. 그 결과로 height, width가 2(1+1, 1+1)씩 추가되기 때문에 convolution을 진행하면서 `kernel size - 1` 만큼 화소가 줄어드는 부분을 상쇄할 수 있다.

이를 일반화하면 kernel size가 n x n 일 때 (n은 주로 odd number) (n - 1) / 2 만큼 padding을 주면 화소가 줄어들지 않는다.

층을 깊게 쌓으니 약간의 개선이 있었음.

<br>

**왜 하필 층이 깊은 모델일 때 최적화 함수의 변경(to Momentum or Adam)이 중요한거임?**

경사도 소실을 비롯하여 **층이 깊어짐으로써 생기는 문제들이 많아져 학습이 효율적으로 이루어지지 않는다**. 따라서 선행 연구자들은 보다 복잡하고 정교한 최적화 알고리즘을 통해 이러한 문제들을 해결하려고 한 것.